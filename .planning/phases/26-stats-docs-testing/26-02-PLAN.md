---
phase: 26-stats-docs-testing
plan: 02
type: execute
---

<objective>
Create unit tests for stats computation logic and add stats methods to the UAT test script.

Purpose: Validate that stats methods correctly aggregate data and handle edge cases, and confirm stats work against the live API — completing the v1.2 quality assurance surface.
Output: Unit test file for stats computation, updated UAT script with stats coverage, all tests passing.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plan in this phase:
@.planning/phases/26-stats-docs-testing/26-01-SUMMARY.md

# Source files under test:
@src/sonnys_data_client/resources/_stats.py
@src/sonnys_data_client/types/_stats.py

# Existing test patterns:
@tests/test_types.py
@tests/test_date_utils.py
@tests/test_resources.py
@test_uat.py

**Tech stack available:** pytest, unittest.mock
**Established patterns:** pytest classes (TestXxx), unittest.mock.patch for mocking client methods, test_date_utils.py for TDD-style tests
**Constraining decisions:**
- Stats methods delegate to _fetch_* helpers which call client.transactions.* and client.recurring.* — mock at the _fetch_* level to test computation logic in isolation
- report() duplicates computation logic from individual methods for efficiency — test independently
- Division-by-zero safe conversion rate when total_opportunities is 0
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create stats computation unit tests</name>
  <files>tests/test_stats.py</files>
  <action>
Create `tests/test_stats.py` with pytest classes testing all stat method computation logic. Mock the `_fetch_*` helper methods on StatsResource to return controlled test data, then verify computation results.

**Test fixtures (module-level or class-level):**
- Create a mock SonnysClient (use `unittest.mock.MagicMock`)
- Instantiate `StatsResource(mock_client)` for each test class
- Use `unittest.mock.patch.object` to mock `_fetch_transactions_v2`, `_fetch_transactions_by_type`, `_fetch_recurring_status_changes` on the StatsResource instance

**Test classes and cases:**

`class TestTotalSales:`
- `test_categorizes_recurring_plan_sales` — v2 transactions with `is_recurring_plan_sale=True` count toward recurring_plan_sales bucket
- `test_categorizes_recurring_redemptions` — v2 transactions with `is_recurring_plan_redemption=True` count toward recurring_redemptions bucket
- `test_categorizes_retail` — v2 transactions with both flags False count toward retail bucket
- `test_totals_all_buckets` — Mix of all 3 types sums correctly for total and count
- `test_empty_transactions` — No transactions returns SalesResult with all zeros

`class TestTotalWashes:`
- `test_counts_wash_and_prepaid` — wash_count from "wash" type, prepaid_wash_count from "prepaid-wash" type, total is sum
- `test_empty_results` — No transactions returns WashResult with all zeros

`class TestRetailWashCount:`
- `test_returns_count_of_wash_type` — Returns len() of "wash" type transactions
- `test_empty_returns_zero` — No transactions returns 0

`class TestNewMembershipsSold:`
- `test_counts_active_transitions` — Only status changes with `new_status == "Active"` are counted
- `test_ignores_non_active_transitions` — "Cancelled", "Suspended" transitions are excluded
- `test_empty_returns_zero` — No status changes returns 0

`class TestConversionRate:`
- `test_computes_rate` — rate = new_memberships / (new_memberships + retail_washes)
- `test_zero_opportunities_returns_zero_rate` — When both are 0, rate is 0.0 (not division error)
- `test_result_fields` — ConversionResult has correct new_memberships, retail_washes, total_opportunities

`class TestReport:`
- `test_computes_all_kpis` — Verify sales, washes, new_memberships, conversion all populated correctly from shared data
- `test_period_dates_as_iso_strings` — period_start and period_end are ISO date strings
- `test_empty_data` — All fetches return empty lists, report has zeros everywhere
- `test_four_fetch_calls_made` — Verify exactly 4 _fetch_* calls made (v2, wash, prepaid-wash, recurring status)

**Mock data construction:**
For TransactionV2ListItem mocks, create simple objects with `is_recurring_plan_sale`, `is_recurring_plan_redemption`, and `total` fields. Use `unittest.mock.MagicMock(spec=TransactionV2ListItem)` with configured attributes, or construct actual model instances with minimal data.

For TransactionListItem mocks, simple MagicMock objects are sufficient since only `len()` is used.

For RecurringStatusChange mocks, create objects with `new_status` field ("Active", "Cancelled", etc.).

**Important:** Mock at the `_fetch_*` method level (patch.object on StatsResource instance), NOT at the client.transactions level. This tests computation logic in isolation from data fetching.
  </action>
  <verify>pytest tests/test_stats.py -v passes all tests</verify>
  <done>All stat methods have computation tests covering normal cases, edge cases (empty data, zero division), and result field correctness</done>
</task>

<task type="auto">
  <name>Task 2: Add stats to UAT script and run full test suite</name>
  <files>test_uat.py</files>
  <action>
Add stats method tests to `test_uat.py` after the existing Transactions section. Use the same `run_test()` pattern with short date ranges for speed.

**Add inside `run_tests(site_code)` function, after the Transactions section:**

```python
# -- Stats ----------------------------------------------------------------
```

Test each stat method with the existing `start_ts`/`end_ts` (1-day range for speed) converted to ISO date strings. Stats methods accept ISO strings directly:

1. `Stats.retail_wash_count()` — Call `client.stats.retail_wash_count(start_date, end_date)`, print count, assert result is int
2. `Stats.new_memberships_sold()` — Call `client.stats.new_memberships_sold(start_date, end_date)`, print count, assert result is int
3. `Stats.total_sales()` — Call `client.stats.total_sales(start_date, end_date)`, print `result.total` and `result.count`, assert isinstance SalesResult
4. `Stats.total_washes()` — Call `client.stats.total_washes(start_date, end_date)`, print `result.total`, assert isinstance WashResult
5. `Stats.conversion_rate()` — Call `client.stats.conversion_rate(start_date, end_date)`, print `result.rate`, assert isinstance ConversionResult
6. `Stats.report()` — Call `client.stats.report(start_date, end_date)`, print summary line with sales total + washes total + conversion rate, assert isinstance StatsReport

Use a 7-day range (existing `start_date`/`end_date` ISO strings) since stats methods accept ISO strings and handle conversion internally.

After adding stats tests, run the full unit test suite:
```bash
pytest tests/ -v
```

Verify all existing + new tests pass.
  </action>
  <verify>pytest tests/ -v passes all tests (existing + new test_stats.py); test_uat.py has stats section with 6 test functions</verify>
  <done>Stats unit tests pass, UAT script includes all 6 stat methods, full pytest suite green</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `tests/test_stats.py` exists with tests for all 6 stat methods
- [ ] `pytest tests/test_stats.py -v` passes all tests
- [ ] `pytest tests/ -v` passes all tests (no regressions)
- [ ] `test_uat.py` includes stats test section with 6 tests
- [ ] Phase 26 complete — all documentation and testing delivered
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No test regressions
- Stats computation logic validated with controlled mock data
- UAT script covers all stats methods for live API validation
- Phase 26 complete, v1.2 milestone ready for close
</success_criteria>

<output>
After completion, create `.planning/phases/26-stats-docs-testing/26-02-SUMMARY.md`
</output>
