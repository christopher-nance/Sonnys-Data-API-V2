---
phase: 27-labor-data-layer
plan: 02
type: execute
---

<objective>
Add bulk clock entry fetching to StatsResource that iterates all employees
across 14-day date chunks, returning a flat list of ClockEntry objects
filtered by site_code.

Purpose: This is the data layer that downstream labor cost and CPC
computations (Phases 28-30) will consume.

Output: Working `_fetch_all_clock_entries()` private method on StatsResource.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/27-labor-data-layer/27-RESEARCH.md
@.planning/phases/27-labor-data-layer/27-01-SUMMARY.md

@src/sonnys_data_client/resources/_stats.py
@src/sonnys_data_client/resources/_employees.py
@src/sonnys_data_client/types/_employees.py
@src/sonnys_data_client/_date_utils.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add _fetch_all_clock_entries to StatsResource</name>
  <files>src/sonnys_data_client/resources/_stats.py</files>
  <action>
    Add a private method `_fetch_all_clock_entries(self, start, end)` to
    StatsResource that:

    1. Imports `build_date_chunks` from `_date_utils` and `ClockEntry` from
       types.
    2. Resolves start/end to YYYY-MM-DD strings (handle both str and datetime
       inputs — extract date portion only, no timezone conversion needed since
       clock entries use YYYY-MM-DD not Unix timestamps).
    3. Calls `build_date_chunks(start_str, end_str)` to get 14-day windows.
    4. Calls `self._client.employees.list()` to get all employee IDs.
    5. For each employee, for each chunk, calls
       `self._client.employees.get_clock_entries(emp.employee_id,
       start_date=chunk_start, end_date=chunk_end)`.
    6. Collects all entries into a flat list.
    7. Filters by `self._client.site_code` if set:
       `[e for e in all_entries if e.site_code == site_code]`.
       If site_code is None, return all entries unfiltered.
    8. Returns `list[ClockEntry]`.

    Add import for ClockEntry at the top of the file alongside existing type
    imports.  Add import for build_date_chunks from _date_utils.

    Do NOT add any public methods yet — that's Phase 29-30.  This is just the
    private data-fetching layer.

    Handle edge case: if an employee has no clock entries for a chunk, the API
    returns empty weeks — the existing get_clock_entries already handles this
    by returning an empty list.

    For date string extraction from datetime inputs: use
    `start.isoformat()[:10] if isinstance(start, datetime) else start` and
    strip any "T" portion.  Do NOT use _resolve_dates (that returns Unix
    timestamps which clock entries don't accept).
  </action>
  <verify>
    python -c "from sonnys_data_client.resources._stats import StatsResource; print('Import OK')"
  </verify>
  <done>
    _fetch_all_clock_entries method exists on StatsResource, accepts start/end
    as str or datetime, returns list[ClockEntry], filters by site_code.
  </done>
</task>

<task type="auto">
  <name>Task 2: Smoke-test with live JOLIET data</name>
  <files>PRODUCTION_TESTS/test_labor_fetch.py</files>
  <action>
    Create a small smoke-test script in PRODUCTION_TESTS/ that:

    1. Creates a SonnysClient for JOLIET (using env vars or hardcoded test
       creds like validate_january_v2.py does).
    2. Calls `client.stats._fetch_all_clock_entries("2026-01-27", "2026-01-31")`
       (5-day range = 1 chunk, small API cost).
    3. Prints: number of employees fetched, number of entries returned,
       total regular hours, total overtime hours, unique site_codes seen.
    4. Asserts at least 1 entry returned (JOLIET is an active site).

    Use a SHORT date range (5 days) to minimize API calls during testing.
    Pattern the script after existing PRODUCTION_TESTS/validate_january_v2.py.
  </action>
  <verify>python PRODUCTION_TESTS/test_labor_fetch.py</verify>
  <done>
    Script runs without error, prints employee count + entry count + hours
    summary, confirms entries are returned for JOLIET.
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `_fetch_all_clock_entries` method exists and imports cleanly
- [ ] Smoke test returns real clock entry data from JOLIET
- [ ] Site code filtering works (only JOLIET entries returned)
- [ ] No regressions in existing stats methods
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- _fetch_all_clock_entries works end-to-end with live API
- Phase 27 data layer complete — ready for Phase 28 (result models)
</success_criteria>

<output>
After completion, create `.planning/phases/27-labor-data-layer/27-02-SUMMARY.md`
</output>
